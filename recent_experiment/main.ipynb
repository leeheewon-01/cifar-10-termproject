{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwlee/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import wandb\n",
    "\n",
    "# Import custom modules\n",
    "from utils.loss import ASLSingleLabel\n",
    "from utils.optim import ASAM, SAM\n",
    "from utils.dataset import Cifar10SearchDataset\n",
    "\n",
    "# Set warning filter\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleeheewon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hwlee/cifar/wandb/run-20230514_232619-oo313jlk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leeheewon/cifa10_proj/runs/oo313jlk' target=\"_blank\">SAM_cosine_ASL_augv3_224</a></strong> to <a href='https://wandb.ai/leeheewon/cifa10_proj' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/leeheewon/cifa10_proj' target=\"_blank\">https://wandb.ai/leeheewon/cifa10_proj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/leeheewon/cifa10_proj/runs/oo313jlk' target=\"_blank\">https://wandb.ai/leeheewon/cifa10_proj/runs/oo313jlk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/leeheewon/cifa10_proj/runs/oo313jlk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7eff3a0769d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['WANDB_START_METHOD'] = 'thread'\n",
    "\n",
    "run_name = 'SAM_cosine_ASL_augv3_224'\n",
    "wandb.init(project=\"cifa10_proj\", name=run_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'Learning_rate': 1e-4,\n",
    "    'EPOCHS': 100,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'SEED' : 42\n",
    "}\n",
    "\n",
    "wandb.config.update(CFG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadaset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "composed_train = A.Compose([A.Resize(224, 224),\n",
    "                            A.Rotate(limit=30, p=0.5),\n",
    "                            A.HorizontalFlip(p=0.2),\n",
    "                            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.1),\n",
    "                            A.Cutout(num_holes=1, max_h_size=16, max_w_size=16, p=0.75),\n",
    "                            A.Normalize(mean=mean, std=std)\n",
    "                            ])\n",
    "\n",
    "composed_test = A.Compose([A.Resize(224, 224),\n",
    "                           A.HorizontalFlip(p=0.2),\n",
    "                           A.Normalize(mean=mean, std=std)\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"../data/CIFAR_10\"\n",
    "\n",
    "train_dataset = Cifar10SearchDataset(root=root_dir, train=True, download=True, transform=composed_train)\n",
    "test_dataset = Cifar10SearchDataset(root=root_dir, train=False, transform=composed_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=CFG['BATCH_SIZE'],\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=32\n",
    "                          )\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=CFG['BATCH_SIZE'],\n",
    "                         shuffle=False,\n",
    "                         pin_memory=True,\n",
    "                         num_workers=32\n",
    "                         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "\n",
    "model = models.resnet18(pretrained='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# resmetV2 - resnet18\n",
    "# model = timm.create_model('resnet18', pretrained=True, num_classes=10).to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=CFG['Learning_rate'], momentum=0.9, weight_decay=1e-4)\n",
    "base_optimizer = torch.optim.AdamW #(model.parameters(), lr=CFG['Learning_rate'])\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=CFG['Learning_rate'])\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class ModelEmaV2(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999, device=None):\n",
    "        super(ModelEmaV2, self).__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device  # perform ema on different device from model if set\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1], dim=0, output_device=0)\n",
    "    model.to(device)\n",
    "\n",
    "    model_ema = ModelEmaV2(model, device=device)\n",
    "\n",
    "    criterion = ASLSingleLabel().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        total, correct = 0, 0\n",
    "        for step, (imgs, labels) in enumerate(tqdm(iter(train_loader)), start=1):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(imgs)\n",
    "\n",
    "            # first step\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                # # optimizer.first_step(zero_grad=True)\n",
    "                # minimizer.ascent_step()\n",
    "\n",
    "                # # second step\n",
    "                # criterion(model(imgs), labels).backward()\n",
    "                # # optimizer.second_step(zero_grad=True)\n",
    "                # minimizer.descent_step()\n",
    "\n",
    "                def closure():\n",
    "                    return criterion(model(imgs), labels).backward()\n",
    "                \n",
    "                optimizer.step(closure)\n",
    "\n",
    "                optimizer.zero_grad()  # Reset gradients after accumulation\n",
    "\n",
    "            model_ema.update(model)\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            _, predicted = output.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch}] Train Loss : [{_train_loss:.5f}] Train Acc [{train_acc:.5f}] Val Loss : [{_val_loss:.5f}] Val Acc : [{_val_score:.5f}]')\n",
    "       \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        wandb.log({\"Val Loss\": _val_loss, \"Val Acc\": _val_score, \"Train Loss\": _train_loss, \"Train Acc\": train_acc, \"lr\" : optimizer.param_groups[0]['lr']})\n",
    "        \n",
    "    print(f'Best Epoch : [{best_epoch}] Best Score : [{best_score:.5f}]')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    totals, corrects = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in iter(val_loader):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            _, predicted = pred.max(1)\n",
    "            totals += labels.size(0)\n",
    "            corrects += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = corrects / totals\n",
    "    \n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 62/391 [00:21<01:52,  2.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hwlee/cifar/main.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m infer_model \u001b[39m=\u001b[39m train(model, optimizer, train_loader, test_loader, exp_lr_scheduler, device)\n",
      "\u001b[1;32m/home/hwlee/cifar/main.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep(closure)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Reset gradients after accumulation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m model_ema\u001b[39m.\u001b[39;49mupdate(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/hwlee/cifar/main.ipynb Cell 19\u001b[0m in \u001b[0;36mModelEmaV2.update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(model, update_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m e, m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecay \u001b[39m*\u001b[39;49m e \u001b[39m+\u001b[39;49m (\u001b[39m1.\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecay) \u001b[39m*\u001b[39;49m m)\n",
      "\u001b[1;32m/home/hwlee/cifar/main.ipynb Cell 19\u001b[0m in \u001b[0;36mModelEmaV2._update\u001b[0;34m(self, model, update_fn)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     model_v \u001b[39m=\u001b[39m model_v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m ema_v\u001b[39m.\u001b[39mcopy_(update_fn(ema_v, model_v))\n",
      "\u001b[1;32m/home/hwlee/cifar/main.ipynb Cell 19\u001b[0m in \u001b[0;36mModelEmaV2.update.<locals>.<lambda>\u001b[0;34m(e, m)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, model):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2274776f475055227d/home/hwlee/cifar/main.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update(model, update_fn\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m e, m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecay \u001b[39m*\u001b[39m e \u001b[39m+\u001b[39m (\u001b[39m1.\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecay) \u001b[39m*\u001b[39;49m m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "infer_model = train(model, optimizer, train_loader, test_loader, exp_lr_scheduler, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "infer_model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(iter(test_loader)):\n",
    "        imgs = imgs.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = infer_model(imgs)\n",
    "\n",
    "        preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "        true_labels += labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "report_df = pd.DataFrame(classification_report(true_labels, preds, target_names=class_names, output_dict=True)).transpose()\n",
    "print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(true_labels, preds)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
