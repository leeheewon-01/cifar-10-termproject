{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import wandb\n",
    "\n",
    "# Import custom modules\n",
    "from utils.loss import ASLSingleLabel\n",
    "from utils.optim import ASAM, SAM\n",
    "from utils.dataset import Cifar10SearchDataset\n",
    "\n",
    "# Set warning filter\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_START_METHOD'] = 'thread'\n",
    "\n",
    "run_name = 'SAM_cosine_ASL_augv3_224'\n",
    "wandb.init(project=\"cifa10_proj\", name=run_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'Learning_rate': 1e-4,\n",
    "    'EPOCHS': 100,\n",
    "    'BATCH_SIZE': 128,\n",
    "    'SEED' : 42\n",
    "}\n",
    "\n",
    "wandb.config.update(CFG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dadaset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "composed_train = A.Compose([A.Resize(224, 224),\n",
    "                            A.Rotate(limit=30, p=0.5),\n",
    "                            A.HorizontalFlip(p=0.2),\n",
    "                            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.1),\n",
    "                            A.Cutout(num_holes=1, max_h_size=16, max_w_size=16, p=0.75),\n",
    "                            A.Normalize(mean=mean, std=std)\n",
    "                            ])\n",
    "\n",
    "composed_test = A.Compose([A.Resize(224, 224),\n",
    "                           A.HorizontalFlip(p=0.2),\n",
    "                           A.Normalize(mean=mean, std=std)\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../data/CIFAR_10\"\n",
    "\n",
    "train_dataset = Cifar10SearchDataset(root=root_dir, train=True, download=True, transform=composed_train)\n",
    "test_dataset = Cifar10SearchDataset(root=root_dir, train=False, transform=composed_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=CFG['BATCH_SIZE'],\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          num_workers=32\n",
    "                          )\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=CFG['BATCH_SIZE'],\n",
    "                         shuffle=False,\n",
    "                         pin_memory=True,\n",
    "                         num_workers=32\n",
    "                         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "\n",
    "model = models.resnet18(pretrained='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# resmetV2 - resnet18\n",
    "# model = timm.create_model('resnet18', pretrained=True, num_classes=10).to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=CFG['Learning_rate'], momentum=0.9, weight_decay=1e-4)\n",
    "base_optimizer = torch.optim.AdamW #(model.parameters(), lr=CFG['Learning_rate'])\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=CFG['Learning_rate'])\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'], eta_min=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class ModelEmaV2(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999, device=None):\n",
    "        super(ModelEmaV2, self).__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device  # perform ema on different device from model if set\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1], dim=0, output_device=0)\n",
    "    model.to(device)\n",
    "\n",
    "    model_ema = ModelEmaV2(model, device=device)\n",
    "\n",
    "    criterion = ASLSingleLabel().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    gradient_accumulation_steps = 1\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        total, correct = 0, 0\n",
    "        for step, (imgs, labels) in enumerate(tqdm(iter(train_loader)), start=1):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(imgs)\n",
    "\n",
    "            # first step\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                # # optimizer.first_step(zero_grad=True)\n",
    "                # minimizer.ascent_step()\n",
    "\n",
    "                # # second step\n",
    "                # criterion(model(imgs), labels).backward()\n",
    "                # # optimizer.second_step(zero_grad=True)\n",
    "                # minimizer.descent_step()\n",
    "\n",
    "                def closure():\n",
    "                    return criterion(model(imgs), labels).backward()\n",
    "                \n",
    "                optimizer.step(closure)\n",
    "\n",
    "                optimizer.zero_grad()  # Reset gradients after accumulation\n",
    "\n",
    "            model_ema.update(model)\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            _, predicted = output.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        print(f'Epoch [{epoch}] Train Loss : [{_train_loss:.5f}] Train Acc [{train_acc:.5f}] Val Loss : [{_val_loss:.5f}] Val Acc : [{_val_score:.5f}]')\n",
    "       \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        wandb.log({\"Val Loss\": _val_loss, \"Val Acc\": _val_score, \"Train Loss\": _train_loss, \"Train Acc\": train_acc, \"lr\" : optimizer.param_groups[0]['lr']})\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    totals, corrects = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in iter(val_loader):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            loss = criterion(pred, labels)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            _, predicted = pred.max(1)\n",
    "            totals += labels.size(0)\n",
    "            corrects += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = corrects / totals\n",
    "    \n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model = train(model, optimizer, train_loader, test_loader, exp_lr_scheduler, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "infer_model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in tqdm(iter(test_loader)):\n",
    "        imgs = imgs.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        pred = infer_model(imgs)\n",
    "\n",
    "        preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "        true_labels += labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "report_df = pd.DataFrame(classification_report(true_labels, preds, target_names=class_names, output_dict=True)).transpose()\n",
    "print(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(true_labels, preds)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='.2f', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
